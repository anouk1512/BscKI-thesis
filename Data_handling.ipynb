{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import statements\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import ast\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Normalizing functions \n",
    "\"\"\"\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "def norm_sent(sentence): \n",
    "    clean = []\n",
    "    clean_sent = sentence.lower().split()\n",
    "    for dirt_word in clean_sent: \n",
    "        word = dirt_word.strip()\n",
    "        clean_word = norm_word(word)\n",
    "        if clean_word not in stoplist: \n",
    "            clean.append(clean_word)\n",
    "    return clean\n",
    "\n",
    "\n",
    "# use this normalizing funtion for information extraction purposed\n",
    "def norm_sentence(sent): \n",
    "    if sent != None: \n",
    "        return sent.lower().split()\n",
    "\n",
    "\n",
    "def stem_word(wrd): \n",
    "    return porter.stem(wrd)\n",
    "\n",
    "\n",
    "def norm_word(wrd): \n",
    "    return re.sub(r'[^a-zA-Z0-9%]', '', stem_word(wrd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Functions for preprocessing CVs \n",
    "\"\"\"\n",
    "\n",
    "def open_file(file): \n",
    "    file = open(file, \"r\")\n",
    "    read_file = file.readlines()\n",
    "    file.close()\n",
    "    return read_file\n",
    "\n",
    "def preprocess_one_file(file): \n",
    "    texts = []\n",
    "    for line in file:\n",
    "        for word in norm_sentence(line): \n",
    "            clean_word = norm_word(word)\n",
    "            if clean_word not in stoplist:\n",
    "                texts.append(clean_word)\n",
    "    return texts \n",
    "\n",
    "def preprocess_more_files(file, signal): \n",
    "    one_cv = []\n",
    "    cvs = []\n",
    "    count = 0 \n",
    "    \n",
    "    for cv_lines in file:\n",
    "        for cv_word in norm_sentence(cv_lines):\n",
    "            clean_word = norm_word(cv_word)\n",
    "            if clean_word not in stoplist: \n",
    "                match = re.search(signal, clean_word)\n",
    "                if match: \n",
    "                    if one_cv != []:\n",
    "                        cvs.append(one_cv)\n",
    "                        one_cv = []\n",
    "                        count += 1 \n",
    "                else:\n",
    "                    one_cv.append(clean_word)\n",
    "    cvs.append(one_cv)\n",
    "    return cvs\n",
    "\n",
    "# use string when using regular expressions \n",
    "def txt_to_str(doc, signal=None): \n",
    "    with open(doc, 'r') as file:\n",
    "        data = file.read().replace('\\n', ' ')\n",
    "        clean_data = data.lower().replace('\\xa0', ' ')\n",
    "    if signal == None: \n",
    "        return str(clean_data)\n",
    "    else: \n",
    "        split_data = re.split(signal, clean_data)\n",
    "        del split_data[0]\n",
    "        return split_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Code for preprocessing document of CVs \n",
    "The file \"linkedin_cvs_10052021\" is extracted by myself using the following search in Google: \n",
    "site:linkedin.com/in/ AND \"data scientist\". The resulting PDFs were tranformed to txt files by hand\n",
    "The CVs are stored as lists in list\n",
    "\"\"\"\n",
    "\n",
    "read_li_file = open_file(\"linkedin_cvs_10052021.txt\")\n",
    "li_cvs_txt = txt_to_str(\"linkedin_cvs_10052021.txt\", 'linkedin-\\d\\d')\n",
    "li_cvs = preprocess_more_files(read_li_file, r'linkedin\\d\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Code for preprocessing real life examples of vacancies \n",
    "These examples are provided by Thom \n",
    "The job title is normalized and stemmed \n",
    "The job description is already normalized, we only have to stem it \n",
    "The result is stored in a dict, the key is the row in the Dataframe\n",
    "so you can extract the name of the company/title of the vacancy\n",
    "The first value is the job title, the second is the job description\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def preprocess_df(dataframe): \n",
    "    vac_dict = dict()\n",
    "    for i in range(1, len(dataframe)+1):\n",
    "        title = dataframe.iat[i-1,0]\n",
    "        numb = dataframe.iat[i-1,1]\n",
    "        if not pd.isna(numb):\n",
    "            stem_vac = []\n",
    "            tot_vac = list(itertools.chain.from_iterable(ast.literal_eval(numb)))\n",
    "            if not pd.isna(title):\n",
    "                clean_title = norm_sent(title)\n",
    "            for wrd in tot_vac: \n",
    "                if wrd not in stoplist: \n",
    "                    stem_vac.append(stem_word(wrd))\n",
    "            vac_dict[i] = [clean_title, stem_vac]\n",
    "\n",
    "    return vac_dict\n",
    "\n",
    "data = pd.read_csv(r'clean_50_job_descriptions.csv')\n",
    "pre_df = pd.DataFrame(data, columns= ['title', 'description'])\n",
    "df = pre_df.drop([0], axis=0)\n",
    "vac_dict = preprocess_df(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
